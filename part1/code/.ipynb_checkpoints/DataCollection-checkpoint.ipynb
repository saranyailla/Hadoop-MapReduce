{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code collects data using NYT Developer API.Creates for text file for each article in the same folder.\n",
    " \n",
    "Takes care of paginated articles.\n",
    "Retrieves urls within the specified date range for the given keywords.\n",
    "Extract paragraph tags for the url and stores them in a text file in the directory specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,json\n",
    "import sys\n",
    "import math\n",
    "\n",
    "api_key=\"xx1Kq913PUG7Eu8q4q5qoIiciGCxwq0x\"\n",
    "\n",
    "s=\"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "\n",
    "#Change these accordingly.Format: yyyymmdd\n",
    "begin_date=\"20190316\"\n",
    "end_date=\"20190331\"\n",
    "\n",
    "#Full list of terms\n",
    "queryTerms=[\"murder\",\"assault\",\"robbery\",\"burglary\",\"theft\",\"stolen\",\"kidnap\",\"homicide\",\"slavery\"]\n",
    "for query in queryTerms:\n",
    "    # URL that takes above parameters\n",
    "    print(query)\n",
    "    inurl=s+\"begin_date=\"+begin_date+\"&end_date=\"+end_date+\"&facet=true&q=\"+query+\"&api-key=\"+api_key\n",
    "    data=urllib.request.urlopen(inurl)\n",
    "    response = json.load(data)['response']\n",
    "    hits = response['meta']['hits']\n",
    "    pages=math.ceil(hits/10)\n",
    "    print(pages)\n",
    "    for page in range(1,pages+1):\n",
    "       # print(page)\n",
    "        inurl=s+\"begin_date=\"+begin_date+\"&end_date=\"+end_date+\"&page=\"+str(page)+\"&facet=true&q=\"+query+\"&api-key=\"+api_key\n",
    "        data=urllib.request.urlopen(inurl)\n",
    "        response = json.load(data)['response']\n",
    "        docs=response['docs']\n",
    "        i=0;\n",
    "        for d in docs:\n",
    "            i=i+1;\n",
    "            url=d['web_url']\n",
    "            try:\n",
    "                resp = requests.get(url)\n",
    "                soup = BeautifulSoup(resp.text, 'lxml')\n",
    "                paragraphs = soup.find_all('p')\n",
    "                paras=\"\"\n",
    "                for p in paragraphs:\n",
    "                    paras = paras + p.get_text()\n",
    "                with open(\"../data/Commoncrawl/\"+query+\"_\"+str(page)+\"_\"+str(i)+\".txt\", 'w') as outfile:  \n",
    "                    outfile.write(paras)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are collecting using R's searchTwitter API.\n",
    "\n",
    "All the tweets specific to a topic are collected into a single file.\n",
    "\n",
    "Only the tweets text present in the tweet object is stored for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key <- '1VKbEcEZKY9iODbH7cORdOHNy'\n",
    "# api_secret <- 'DpWgTXK9xzzbrDn7wLSAM8SCDvz1t26viUpNTxLqCucsUDbPI6'\n",
    "# access_token <- '1096842973654990848-o0Z0dRW5SO1HhsKeO1oGVKVknTAS16'\n",
    "# access_token_secret <- '0OtRs7qZ5yN0TW20Q30h0jQyR0lteBp60mJeoRwaDH6nZ'\n",
    "# setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)\n",
    "\n",
    "api_key <-  \"E8dLYeLHXpgKoeQejPfMozLqQ\"\n",
    "api_secret <-  \"WKCyYMvJqlzDkhSwB3LPlwfrksJ6pauGLJoNidWIz8HwYkEsS1\"\n",
    "access_token <-  \"238578759-GJcMJFLrIc0EBsCRDPXAb87iC3PoRdYU5PbPbSh8\"\n",
    "access_token_secret <- \"2u3lM65eWJLGBzly4IbiAqjxCyC7Gox2Mx7EoG5rEK8pF\"\n",
    "setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)\n",
    "key<-\"crime\"\n",
    "rt <- searchTwitter(key+\" -filter:retweets\", n = 5000, lang='en', since='2019-01-01', geocode='40.482405,-97.413745,2280mi')\n",
    "rt.df <-twListToDF(rt)\n",
    "write.csv(rt.df$text,\"../data/Twitter/\"+key+\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommonCrawl Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract warc from a gz file\n",
    "\n",
    "#crawl-data/CC-NEWS/2019/04/CC-NEWS-20190414063114-00197.warc.gz\n",
    "#crawl-data/CC-NEWS/2019/04/CC-NEWS-20190413184753-00195.warc.gz\n",
    "#crawl-data/CC-NEWS/2019/04/CC-NEWS-20190415106109-00198.warc.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open('/path/to/the/.gz/file/', 'rb') as f_in:\n",
    "    with open('warcName.warc', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code collects only those urls of the webpages that contains the specified keywords in their paragraph tags and store them in the csv file.\n",
    "\n",
    "These csv files are specific to the topic we are collecting.\n",
    "\n",
    "We have extracted files using three warc.gz files of news Dataset of commoncrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError\n",
    "import time\n",
    "import csv\n",
    "import warc\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "#crawl-data/CC-NEWS/2019/04/CC-NEWS-20190414063114-00197.warc.gz\n",
    "#crawl-data/CC-NEWS/2019/04/CC-NEWS-20190413184753-00195.warc.gz\n",
    "#crawl-data/CC-NEWS/2019/04/CC-NEWS-20190412131544-00187.warc.gz-- to be crawled\n",
    "n=0;\n",
    "f = warc.open(\"warcName.warc\")\n",
    "c=0;\n",
    "mycolumns = ['links']\n",
    "murder = pd.DataFrame(columns=mycolumns)\n",
    "abuse = pd.DataFrame(columns=mycolumns)\n",
    "kidnap = pd.DataFrame(columns=mycolumns)\n",
    "theft = pd.DataFrame(columns=mycolumns)\n",
    "stolen = pd.DataFrame(columns=mycolumns)\n",
    "slavery = pd.DataFrame(columns=mycolumns)\n",
    "burglary = pd.DataFrame(columns=mycolumns)\n",
    "rape = pd.DataFrame(columns=mycolumns)\n",
    "assault = pd.DataFrame(columns=mycolumns)\n",
    "robbery = pd.DataFrame(columns=mycolumns)\n",
    "for record in f:\n",
    "    try:\n",
    "        if(record.header['WARC-Type']=='response'):\n",
    "            link1=record.header['WARC-TARGET-URI']\n",
    "            resp = requests.get(link1)\n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            paras=\"\"\n",
    "            for p in paragraphs:\n",
    "                paras = paras + p.get_text()            \n",
    "            n=n+1\n",
    "            print(n)\n",
    "            if \"murder\" in paras:\n",
    "                murder.loc[len(murder)]=link1\n",
    "            if \"abuse\" in paras:\n",
    "                abuse.loc[len(abuse)]=link1\n",
    "            if \"kidnap\" in paras:\n",
    "                kidnap.loc[len(kidnap)]=link1\n",
    "            if \"theft\" in paras:\n",
    "                theft.loc[len(theft)]=link1\n",
    "            if \"stolen\" in paras:\n",
    "                stolen.loc[len(stolen)]=link1\n",
    "            if \"burglary\" in paras:\n",
    "                burglary.loc[len(burglary)]=link1\n",
    "            if \"rape\" in paras:\n",
    "                rape.loc[len(rape)]=link1\n",
    "            if \"assault\" in paras:\n",
    "                assault.loc[len(assault)]=link1\n",
    "            if \"robbery\" in paras:\n",
    "                robbery.loc[len(robbery)]=link1\n",
    "            if \"slavery\" in paras:\n",
    "                slavery.loc[len(slavery)]=link1\n",
    "        murder.to_csv(\"murder.csv\", sep='\\t',index = False)\n",
    "        abuse.to_csv(\"abuse.csv\", sep='\\t',index = False)\n",
    "        kidnap.to_csv(\"kidnap.csv\", sep='\\t',index = False)\n",
    "        theft.to_csv(\"theft.csv\", sep='\\t',index = False)\n",
    "        stolen.to_csv(\"stolen.csv\", sep='\\t',index = False)\n",
    "        burglary.to_csv(\"burglary.csv\", sep='\\t',index = False)\n",
    "        rape.to_csv(\"rape.csv\", sep='\\t',index = False)\n",
    "        assault.to_csv(\"assault.csv\", sep='\\t',index = False)\n",
    "        robbery.to_csv(\"robbery.csv\", sep='\\t',index = False)\n",
    "        slavery.to_csv(\"slavery.csv\", sep='\\t',index = False)\n",
    "        \n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        #time.sleep(2)\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code removes the duplicate urls and store each of the webpage's data in an individual file. (One file for each url)\n",
    "\n",
    "We extracted content in the paragraph tags from a webpage using beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,json\n",
    "import sys\n",
    "import math\n",
    "import csv \n",
    "import os\n",
    "seen = set()\n",
    "\n",
    "allFiles=os.listdir(\"\\\\local\\\\path\\\\to\\\\the\\\\csv\\\\files\")\n",
    "for urlFile in allFiles:\n",
    "    urlFilePath=\"\\\\local\\\\path\\\\to\\\\the\\\\csv\\\\files\"+urlFile\n",
    "    with open(urlFilePath, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        result = []\n",
    "        i=0\n",
    "        for url in reader:\n",
    "            url=url[0]\n",
    "            if(url[len(url)-1]=='/'):\n",
    "                k=url.rsplit('/', 2)[1]\n",
    "            else:\n",
    "                k=url.rsplit('/', 1)[-1]\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                result.append(url)\n",
    "                i=i+1\n",
    "                try:\n",
    "                    print(url)\n",
    "                    resp = requests.get(url)\n",
    "                    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "                    paragraphs = soup.find_all('p')\n",
    "                    paras=\"\"\n",
    "                    for p in paragraphs:\n",
    "                        paras = paras + p.get_text()\n",
    "                    with open(\"cc_\"+urlFile+\"_\"+str(i)+\".txt\", 'w') as outfile:  \n",
    "                        outfile.write(paras)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to combine all the files present in the given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "allFiles=os.listdir(\"C:\\\\Users\\\\shara\\\\Desktop\\\\DIC\\\\CommonCrawl\\\\allCC\")\n",
    "print(allFiles)\n",
    "\n",
    "with open('allcc.txt', 'w',encoding='utf8') as outfile:\n",
    "    \n",
    "    for fname in allFiles:\n",
    "        fname=\"C:\\\\Users\\\\shara\\\\Desktop\\\\DIC\\\\CommonCrawl\\\\allCC\\\\\"+fname\n",
    "        with open(fname,encoding='unicode_escape') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
